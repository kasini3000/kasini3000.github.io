# ---【前言】---


问题来自于博客园，讨论容器编排，跨集群部署（脚踩2只船）问题。

# ---【（脚踩2只船）问题，K8s的解决方案】---

博客园提出“假如整个 kubernetes 集群宕机怎么办？”

在我的理解，问题可以看成“k8s的master崩溃咋办？”

这个问题真实存在的。请看：

问：“如何删除namespace下的所有的资源？就是namespace 和她下面的资源 pod development 这些。目前删除后一直重新恢复？”

老司机都知道，是会有这类的问题。因为这里环节太多：

go调用了http，又调用了其他pod，和端口，最终又调用了etcd数据库。

问：“k8s的master崩溃咋办？”

答：用多master，或重启master的机子呗。多master理论上还有etcd同步失败问题。还必须遵守分布式约束【3master坏1，5master坏2】坏多了，整个就瘫痪了。

问：“k8s的master崩溃咋办？”

答：用多个集群也行。就像文章开头博客园那样。不过，我个人觉得，应该是建立2套，毫无关联的集群k8s集群，然后从外部lb，给这两个集群转发流量。这里以博客园为例：

原来：

阿里云lb---》k8s master1-》master1内部的负载均衡，master1内部的dns---》pod

现在：

阿里云lb---》k8s master1-》master1内部的负载均衡，master1内部的dns---》pod

阿里云lb---》k8s master2-》master1内部的负载均衡，master2内部的dns---》pod

这样就达到了，文章最初的目的：容器编排跨集群部署（脚踩2只船）。下面我来谈谈，k4t对这个问题的解法。

# ---【（脚踩2只船）问题，k4t的解决方案】---

问：“k4t的master崩溃咋办？”

答：k4t基本不会崩溃。理由如下：

1 K4t调用的环节少。Linux文件目录，坏的几率比较小。K4t的数据库是一堆【目录】和【xml文本文件】组成的树形目录结构。请问你家linux的文件或目录，崩吧总坏吗？退一万步说，文件目录坏了，也好修理恢复啊。

2 通过定期同步数据库目录，到master2，即可实现【单个集群k4t高可用】。比k8s集群至少3台机子节省硬件。还没有【3坏1，5坏2】约束。

3 k4t不用虚拟网络veth，没有相关组件，天生就比k8s稳定。

4 程序代码引擎是linux版powershell，powershell进程挂了，重启一个powershell即可。就是这么简单粗暴。

5 根据下面【附录：k4t组件介绍】中的说明，k4t的master机，实际上称作【部署机子】更合适。K4t的部署机子，部署完应用后，即可关机。待下次需要变更部署，或者新增、删除、部署时再开机即可。也就是说k4t-master机挂了无所谓。

而k4t真正的master机，其实是负载均衡机。需要对负载均衡机进行高可用，具体做法，在本篇文章中，就不多谈了。

6 k8s的ip和pod隐藏于集群内部，必须为每个集群部署监控，如普罗米休斯。而多套k4t集群，可以用同一个zabbix监控，因为k4t的ip暴露在局域网。K4t更集群轻量。

问：容器编排,跨集群部署（脚踩2只船）。k4t和k8s有啥区别？

答：根据下面【附录：k4t组件介绍】中的说明，k4t的负载均衡机（envoy api-getway）是在外部的。k4t的dns机也是在外部的。这就是k4t集群，和k8s集群，去解决（脚踩2只船）问题最大的不同。

下面，我以脚踩3只船，来讲解k4t的跨集群部署，和k8s的区别。

系统架构：

集群1：master1，带node1，node2

集群2：master2，带node3，node4

集群3：master3，带node5，node6

集群123，共用同一个【负载均衡器1】。共用同一个【dns1】。

用户流量：

阿里云lb---》【k4t负载均衡器1】---》node1，node2，node3上的ip---》node1，node2，node3上的容器

阿里云lb---》【k4t dns1】---》node1，node2，node3上的ip---》node1，node2，node3上的容器

需要说明的是：k4t的dns，是非必要组件。K4t集群，可以只用负载均衡api-getway。

命令目的：把当集群1内，应用1，的容器1，的ip，注册到负载均衡集群。

命令：

```
ssh root@你的k4y的负载均衡器的ip pwsh -f /etc/envoy_ps1/add-eds.ps1 -cname 'abc1' -ip 1.2.3.4 -port 80
```

说明：

1 这个命令，由【在node上的k4t-node引擎】发出，被k4t负载均衡器接收。与用户无关，用户也不需要写任何代码。

2 命令不分集群，2个集群的命令没差别。

3 -cname 'abc1' ，代表应用1的名字。2个集群的命令没差别。当然了，这里算是好处。当然也有坏处，2个集群的2个应用，若不是同一应用，则不应该重名。

4 -ip 1.2.3.4 代表容器1的ip。2个集群的命令没差别。当然了，2个集群的2个容器，ip必须不同。这个值来自于node内的ip设定。2个集群内，Node内的ip不能重复。

结论：

K4t的负载均衡器，和k4t的dns，具有同时对多个【k4t容器集群】服务的能力。

对【多个k4t容器集群】和对【单个k4t容器集群】操作，完全相同！不需要任何额外的设置。

# ---【附录：k4t简介】---

k4t即：《kaiiit生产级别的容器编排系统》

[https://gitee.com/chuanjiao10/k4t](https://gitee.com/chuanjiao10/k4t)

预计2021年6月发布alphi1版

中文名：《海贼帝·优秀的怕被沽沟暗杀·黑毛腿》

中文别名：《ps1倚天剑》

![](https://img2020.cnblogs.com/blog/456691/202101/456691-20210125202544886-71634545.jpg)

这是一套基于《卡死你3000》打造的，全功能，容器集群的自动化部署、扩容以及运维的平台。（国产k8s）。它采用开放式架构，具有网络简单稳定，使用简单，脚本操控灵活的特点。

powershell传教士 业余时间 作品

群名称：k4t官方1群  群   号：722528388

# ---【附录：k4t 组件介绍】---

问：完美版的k4t项目，由哪几部分组件构成？

答：

1 master端。 正在开发中，预计2021年6月前发布alphi1版。

2 node端。已经开发完成，并部分开源免费成为雏形版。

3 动态负载均衡api网关。（外部）

提供反向代理，和负载均衡功能。用于给一组容器应用，形成统一的ip。

目前基于我编写的envoy后端增删脚本。

https://gitee.com/chuanjiao10/envoy\_powershell

你也可diy，改成自己喜欢的。

4 动态dns。（外部）

是非必要组件。K4t集群，可以只用负载均衡器api-getway。

用于给一组容器应用，形成统一的服务名字。

目前基于未激活的无图win2019。当然有图版win2019也行。你也可diy，改成自己喜欢的。例如：dnsmasq

# ---【后记】---

我滴妈呀，这个世界变化太快，微服务，容器都要脚踩2只船了，谢谢观看。

分类: [kaiiit即k4t](https://www.cnblogs.com/piapia/category/1921541.html)

标签: [容器](https://www.cnblogs.com/piapia/tag/%E5%AE%B9%E5%99%A8/) , [负载均衡](https://www.cnblogs.com/piapia/tag/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/) , [powershell](https://www.cnblogs.com/piapia/tag/powershell/) , [linux](https://www.cnblogs.com/piapia/tag/linux/) , [docker](https://www.cnblogs.com/piapia/tag/docker/) , [api网关](https://www.cnblogs.com/piapia/tag/api%E7%BD%91%E5%85%B3/) , [微服务](https://www.cnblogs.com/piapia/tag/%E5%BE%AE%E6%9C%8D%E5%8A%A1/) , [k8s](https://www.cnblogs.com/piapia/tag/k8s/) , [容器编排系统](https://www.cnblogs.com/piapia/tag/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E7%B3%BB%E7%BB%9F/)
